{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Streamlit locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing spam_filter.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile spam_filter.py\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import joblib\n",
    "import streamlit as st\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def clean_text(text):\n",
    "    import re\n",
    "    from string import punctuation\n",
    "    text=re.sub(r'(http|ftp|https):\\/\\/([\\w\\-_]+(?:(?:\\.[\\w\\-_]+)+))([\\w\\-\\.,@?^=%&:/~\\+#]*[\\w\\-\\@?^=%&/~\\+#])?', \n",
    "                ' ', text)\n",
    "    text=re.sub(r'['+punctuation+']',' ',text)\n",
    "    text=re.sub(r'#(\\w+)',' ',text)\n",
    "    text=re.sub(r'@(\\w+)',' ',text)\n",
    "    text = text.lower() # Convert  to lowercase\n",
    "\n",
    "    token=RegexpTokenizer(r'\\w+')\n",
    "    tokens = token.tokenize(text)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stems = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    stemmer = PorterStemmer()\n",
    "    stems = [stemmer.stem(t) for t in stems]\n",
    "    \n",
    "    return ' '.join(stems)\n",
    "\n",
    "def tokenize(text):\n",
    "    token=RegexpTokenizer(r'\\w+')\n",
    "    tokens = token.tokenize(text)\n",
    "    \n",
    "    return tokens    \n",
    "\n",
    "def load_models():     \n",
    "    # Load the vectorizer.\n",
    "    file = open(\"C:/Users/Asus/PYTHON_C/DLBDSME01/spam_filter/src/code_model_deploy/vectorizer.pkl\", 'rb')\n",
    "    vectorizer = joblib.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    # Load the LR Model.\n",
    "    file = open(\"C:/Users/Asus/PYTHON_C/DLBDSME01/spam_filter/src/code_model_deploy/model.pkl\", 'rb')\n",
    "    model = joblib.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    return vectorizer, model\n",
    "\n",
    "df = pd.read_csv('C:/Users/Asus/PYTHON_C/DLBDSME01/spam_filter/data/processed/SMSSpamColl.csv', encoding='utf-8')\n",
    "cv=TfidfVectorizer(lowercase=True,preprocessor=clean_text,stop_words='english',ngram_range=(1,3),tokenizer=tokenize)\n",
    "text_counts=cv.fit_transform(df['text'].values.astype('U'))\n",
    "x_train, x_test, y_train, y_test = train_test_split(text_counts,df['label'],test_size=0.3)\n",
    "\n",
    "def main():    \n",
    "    st.title(\"Spam Classifier\")\n",
    "    st.write(\"Enter your message to check if it's spam or not.\")\n",
    "    user_input = st.text_input(\"Enter message here:\")\n",
    "    \n",
    "    if st.button(\"Check\"):                  \n",
    "    # Make predictions\n",
    "        vectorizer, model = load_models()\n",
    "        model.fit(x_train, y_train) \n",
    "        clean_input = clean_text(user_input)\n",
    "        input_counts = vectorizer.transform([clean_input])\n",
    "        prediction = model.predict(input_counts)[0]\n",
    "        st.write(\"Prediction: \", prediction)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Streamlit to the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spam_classifier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile spam_classifier.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import joblib\n",
    "import streamlit as st\n",
    "import unicodedata\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def clean_text(text):\n",
    "    import re\n",
    "    from string import punctuation\n",
    "    text=re.sub(r'(http|ftp|https):\\/\\/([\\w\\-_]+(?:(?:\\.[\\w\\-_]+)+))([\\w\\-\\.,@?^=%&:/~\\+#]*[\\w\\-\\@?^=%&/~\\+#])?', \n",
    "                ' ', text)\n",
    "    text=re.sub(r'['+punctuation+']',' ',text)\n",
    "    text=re.sub(r'#(\\w+)',' ',text)\n",
    "    text=re.sub(r'@(\\w+)',' ',text)\n",
    "    text = text.lower() # Convert  to lowercase\n",
    "\n",
    "    token=RegexpTokenizer(r'\\w+')\n",
    "    tokens = token.tokenize(text)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stems = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    stemmer = PorterStemmer()\n",
    "    stems = [stemmer.stem(t) for t in stems]\n",
    "    \n",
    "    return ' '.join(stems)\n",
    "\n",
    "def tokenize(text):\n",
    "    token=RegexpTokenizer(r'\\w+')\n",
    "    tokens = token.tokenize(text)\n",
    "    \n",
    "    return tokens    \n",
    "\n",
    "def load_models():     \n",
    "    # Load the vectorizer.\n",
    "    file = open('vectorizer.pkl', 'rb')\n",
    "    vectorizer = joblib.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    # Load the LR Model.\n",
    "    file = open('model.pkl', 'rb')\n",
    "    model = joblib.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    return vectorizer, model\n",
    "\n",
    "df = pd.read_csv('SMSSpamColl.csv', encoding='utf-8')\n",
    "cv=TfidfVectorizer(lowercase=True,preprocessor=clean_text,stop_words='english',ngram_range=(1,3),tokenizer=tokenize)\n",
    "text_counts=cv.fit_transform(df['text'].values.astype('U'))\n",
    "x_train, x_test, y_train, y_test = train_test_split(text_counts,df['label'],test_size=0.3)\n",
    "\n",
    "def main():    \n",
    "    st.title(\"Spam Classifier\")\n",
    "    st.write(\"Enter your message to check if it's spam or not.\")\n",
    "    user_input = st.text_input(\"Enter message here:\")\n",
    "    \n",
    "    if st.button(\"Check\"):                  \n",
    "    # Make predictions\n",
    "        vectorizer, model = load_models()\n",
    "        model.fit(x_train, y_train) \n",
    "        clean_input = clean_text(user_input)\n",
    "        input_counts = vectorizer.transform([clean_input])\n",
    "        prediction = model.predict(input_counts)[0]\n",
    "        st.write(\"Prediction: \", prediction)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tweet sentiment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
